#!/usr/bin/env python3
"""
è¯¦ç»†åˆ†ææœ€åä¸€ä¸ªæ¶ˆæ¯çš„å†…å®¹
æ‰¾å‡ºå¯èƒ½å¯¼è‡´500é”™è¯¯çš„å…·ä½“åŸå› 
"""

import json
import sys
import re
from typing import Dict, Any, List

def find_messages_in_json(data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """é€’å½’æŸ¥æ‰¾JSONä¸­çš„messageså­—æ®µ"""
    if isinstance(data, dict):
        if "messages" in data and isinstance(data["messages"], list):
            return data["messages"]
        
        for key, value in data.items():
            if isinstance(value, dict):
                result = find_messages_in_json(value)
                if result:
                    return result
            elif isinstance(value, list):
                for item in value:
                    if isinstance(item, dict):
                        result = find_messages_in_json(item)
                        if result:
                            return result
    
    return []

def analyze_content(content: str) -> Dict[str, Any]:
    """
    è¯¦ç»†åˆ†ææ¶ˆæ¯å†…å®¹
    
    Args:
        content: æ¶ˆæ¯å†…å®¹
        
    Returns:
        åˆ†æç»“æœ
    """
    analysis = {
        "length": len(content),
        "lines": len(content.split('\n')),
        "special_chars": {},
        "potential_issues": [],
        "content_preview": content[:200] + "..." if len(content) > 200 else content
    }
    
    # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦
    special_chars = {
        '\\t': content.count('\t'),
        '\\n': content.count('\n'),
        '\\r': content.count('\r'),
        '\\\\': content.count('\\\\'),
        '%': content.count('%'),
        '&': content.count('&'),
        '<': content.count('<'),
        '>': content.count('>'),
        '"': content.count('"'),
        "'": content.count("'"),
        '[': content.count('['),
        ']': content.count(']'),
        '(': content.count('('),
        ')': content.count(')'),
        '{': content.count('{'),
        '}': content.count('}'),
        '!': content.count('!'),
        '#': content.count('#'),
        '*': content.count('*'),
        '_': content.count('_'),
        '`': content.count('`'),
        '|': content.count('|'),
        '~': content.count('~'),
        '^': content.count('^'),
        '+': content.count('+'),
        '=': content.count('='),
        '@': content.count('@'),
        '$': content.count('$'),
        ';': content.count(';'),
        ':': content.count(':'),
        ',': content.count(','),
        '.': content.count('.'),
        '?': content.count('?'),
        '/': content.count('/'),
        '\\': content.count('\\'),
    }
    
    analysis["special_chars"] = {k: v for k, v in special_chars.items() if v > 0}
    
    # æ£€æŸ¥æ½œåœ¨é—®é¢˜
    issues = []
    
    # 1. æ£€æŸ¥URLæ ¼å¼
    url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
    urls = re.findall(url_pattern, content)
    if urls:
        analysis["urls"] = urls
        issues.append(f"åŒ…å« {len(urls)} ä¸ªURL")
    
    # 2. æ£€æŸ¥Markdowné“¾æ¥æ ¼å¼
    md_link_pattern = r'\[([^\]]+)\]\(([^)]+)\)'
    md_links = re.findall(md_link_pattern, content)
    if md_links:
        analysis["markdown_links"] = md_links
        issues.append(f"åŒ…å« {len(md_links)} ä¸ªMarkdowné“¾æ¥")
    
    # 3. æ£€æŸ¥è½¬ä¹‰å­—ç¬¦
    escape_pattern = r'\\[^\\]'
    escapes = re.findall(escape_pattern, content)
    if escapes:
        analysis["escape_sequences"] = escapes
        issues.append(f"åŒ…å« {len(escapes)} ä¸ªè½¬ä¹‰åºåˆ—")
    
    # 4. æ£€æŸ¥ç‰¹æ®Šæ ¼å¼
    if content.startswith("name:\t"):
        issues.append("ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”çš„æ ¼å¼")
    
    if "![Python]" in content:
        issues.append("åŒ…å«Markdownå›¾ç‰‡è¯­æ³•")
    
    if "\\\\" in content:
        issues.append("åŒ…å«åŒåæ–œæ ")
    
    if "%2B" in content:
        issues.append("åŒ…å«URLç¼–ç å­—ç¬¦")
    
    # 5. æ£€æŸ¥å†…å®¹ç»“æ„
    lines = content.split('\n')
    if len(lines) > 50:
        issues.append("å†…å®¹è¡Œæ•°è¿‡å¤š")
    
    # 6. æ£€æŸ¥ä¸­æ–‡å­—ç¬¦
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', content)
    if chinese_chars:
        analysis["chinese_chars_count"] = len(chinese_chars)
        issues.append(f"åŒ…å« {len(chinese_chars)} ä¸ªä¸­æ–‡å­—ç¬¦")
    
    # 7. æ£€æŸ¥ç‰¹æ®Šç¬¦å·ç»„åˆ
    if "![Python](https://img.shields.io/badge/Python-3.12%2B-blue.svg)" in content:
        issues.append("åŒ…å«å¤æ‚çš„Markdownå¾½ç« è¯­æ³•")
    
    if "venv\\\\Scripts\\\\activate" in content:
        issues.append("åŒ…å«Windowsè·¯å¾„æ ¼å¼")
    
    analysis["potential_issues"] = issues
    
    return analysis

def analyze_last_tool_message(messages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    åˆ†ææœ€åä¸€ä¸ªtoolæ¶ˆæ¯
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        
    Returns:
        åˆ†æç»“æœ
    """
    if not messages:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°æ¶ˆæ¯"}
    
    # æ‰¾åˆ°æœ€åä¸€ä¸ªtoolæ¶ˆæ¯
    last_tool_index = -1
    for i in range(len(messages) - 1, -1, -1):
        if messages[i].get("role") == "tool":
            last_tool_index = i
            break
    
    if last_tool_index == -1:
        return {"error": "æ²¡æœ‰æ‰¾åˆ°toolæ¶ˆæ¯"}
    
    last_message = messages[last_tool_index]
    content = last_message.get("content", "")
    
    analysis = {
        "message_index": last_tool_index,
        "message_role": last_message.get("role"),
        "tool_call_id": last_message.get("tool_call_id"),
        "name": last_message.get("name"),
        "content_analysis": analyze_content(content)
    }
    
    return analysis

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 analyze_last_message.py <input_file>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    try:
        # è¯»å–æ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8') as f:
            request_body = json.load(f)
        
        print(f"ğŸ“– åˆ†ææ–‡ä»¶: {input_file}")
        
        # æŸ¥æ‰¾messageså­—æ®µ
        messages = find_messages_in_json(request_body)
        
        if not messages:
            print("âš ï¸  æœªæ‰¾åˆ°messageså­—æ®µ")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ°æ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        # åˆ†ææœ€åä¸€ä¸ªtoolæ¶ˆæ¯
        analysis = analyze_last_tool_message(messages)
        
        if "error" in analysis:
            print(f"âŒ {analysis['error']}")
            return
        
        # è¾“å‡ºåˆ†æç»“æœ
        print("\n" + "="*60)
        print("ğŸ” æœ€åä¸€ä¸ªToolæ¶ˆæ¯è¯¦ç»†åˆ†æ")
        print("="*60)
        
        print(f"ğŸ“‹ æ¶ˆæ¯ç´¢å¼•: {analysis['message_index']}")
        print(f"ğŸ“‹ æ¶ˆæ¯è§’è‰²: {analysis['message_role']}")
        print(f"ğŸ“‹ å·¥å…·è°ƒç”¨ID: {analysis['tool_call_id']}")
        print(f"ğŸ“‹ å·¥å…·åç§°: {analysis['name']}")
        
        content_analysis = analysis['content_analysis']
        print(f"\nğŸ“Š å†…å®¹ç»Ÿè®¡:")
        print(f"   - æ€»é•¿åº¦: {content_analysis['length']} å­—ç¬¦")
        print(f"   - æ€»è¡Œæ•°: {content_analysis['lines']} è¡Œ")
        
        if content_analysis['chinese_chars_count']:
            print(f"   - ä¸­æ–‡å­—ç¬¦: {content_analysis['chinese_chars_count']} ä¸ª")
        
        print(f"\nğŸ”§ ç‰¹æ®Šå­—ç¬¦ç»Ÿè®¡:")
        for char, count in content_analysis['special_chars'].items():
            print(f"   - {char}: {count} ä¸ª")
        
        print(f"\nâš ï¸  æ½œåœ¨é—®é¢˜:")
        for issue in content_analysis['potential_issues']:
            print(f"   - {issue}")
        
        if 'urls' in content_analysis:
            print(f"\nğŸ”— å‘ç°çš„URL:")
            for url in content_analysis['urls'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   - {url}")
            if len(content_analysis['urls']) > 3:
                print(f"   - ... è¿˜æœ‰ {len(content_analysis['urls']) - 3} ä¸ªURL")
        
        if 'markdown_links' in content_analysis:
            print(f"\nğŸ“ Markdowné“¾æ¥:")
            for text, url in content_analysis['markdown_links'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   - [{text}]({url})")
            if len(content_analysis['markdown_links']) > 3:
                print(f"   - ... è¿˜æœ‰ {len(content_analysis['markdown_links']) - 3} ä¸ªé“¾æ¥")
        
        print(f"\nğŸ“„ å†…å®¹é¢„è§ˆ:")
        print(f"   {content_analysis['content_preview']}")
        
        print("\n" + "="*60)
        print("ğŸ’¡ å»ºè®®:")
        
        # æ ¹æ®åˆ†æç»“æœç»™å‡ºå»ºè®®
        if content_analysis['length'] > 1000:
            print("   - å†…å®¹è¿‡é•¿ï¼Œè€ƒè™‘æˆªæ–­")
        
        if 'urls' in content_analysis and len(content_analysis['urls']) > 5:
            print("   - URLæ•°é‡è¿‡å¤šï¼Œå¯èƒ½å½±å“è§£æ")
        
        if 'markdown_links' in content_analysis and len(content_analysis['markdown_links']) > 3:
            print("   - Markdowné“¾æ¥è¿‡å¤šï¼Œå¯èƒ½å½±å“æ ¼å¼")
        
        if "![Python]" in content_analysis['content_preview']:
            print("   - åŒ…å«Markdownå›¾ç‰‡è¯­æ³•ï¼Œå¯èƒ½å¯¼è‡´è§£æé”™è¯¯")
        
        if "\\\\" in content_analysis['content_preview']:
            print("   - åŒ…å«åŒåæ–œæ ï¼Œå¯èƒ½å¯¼è‡´è½¬ä¹‰é—®é¢˜")
        
        if "%2B" in content_analysis['content_preview']:
            print("   - åŒ…å«URLç¼–ç å­—ç¬¦ï¼Œå¯èƒ½å¯¼è‡´è§£ç é—®é¢˜")
        
        print("="*60)
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ é”™è¯¯: JSON æ ¼å¼é”™è¯¯ - {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 